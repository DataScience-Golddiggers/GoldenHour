{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import Libraries and Check Device\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(f\"✓ Running on CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Running on Apple Silicon (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"⚠ Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Load and Prepare Data\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/gold_silver.csv')\n",
    "\n",
    "# Convert to datetime and set proper frequency\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.sort_values('DATE')\n",
    "df.set_index('DATE', inplace=True)\n",
    "df = df.asfreq('B')  # Business day frequency\n",
    "\n",
    "# Calculate log returns\n",
    "df['GOLD_LOG_RETURN'] = np.log(df['GOLD_PRICE']) - np.log(df['GOLD_PRICE'].shift(1))\n",
    "\n",
    "print(f\"Dataset: {len(df)} observations\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08346e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Feature Engineering (Technical Indicators Only)\n",
    "# Create feature dataframe\n",
    "features_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "# 1. Target variable (endogenous)\n",
    "features_df['GOLD_LOG_RETURN'] = df['GOLD_LOG_RETURN']\n",
    "\n",
    "# 2. Technical Indicators (LAGGED by 1 period to avoid leakage)\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "def calculate_rsi(prices, period=14):\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "features_df['RSI_LAGGED'] = calculate_rsi(df['GOLD_PRICE']).shift(1)\n",
    "\n",
    "# Simple Moving Average (20-day)\n",
    "features_df['SMA20_LAGGED'] = df['GOLD_PRICE'].rolling(20).mean().shift(1)\n",
    "\n",
    "# Volatility (20-day rolling std of log returns)\n",
    "features_df['VOLATILITY_LAGGED'] = df['GOLD_LOG_RETURN'].rolling(20).std().shift(1)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "features_df = features_df.dropna()\n",
    "\n",
    "print(f\"✓ Features engineered (No GPRD)\")\n",
    "print(f\"Features used: {features_df.columns.tolist()}\")\n",
    "print(f\"Final dataset size: {len(features_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd836e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Visualize Technical Indicators\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Price vs SMA\n",
    "axes[0].plot(df.index, df['GOLD_PRICE'], label='Gold Price', color='gold')\n",
    "axes[0].plot(features_df.index, features_df['SMA20_LAGGED'].shift(-1), label='SMA 20', color='blue', linestyle='--') # Shift back for viz alignment\n",
    "axes[0].set_title('Gold Price vs SMA 20', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# RSI\n",
    "axes[1].plot(features_df.index, features_df['RSI_LAGGED'], label='RSI', color='purple')\n",
    "axes[1].axhline(70, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(30, color='green', linestyle='--', alpha=0.5)\n",
    "axes[1].set_title('Relative Strength Index (RSI)', fontweight='bold')\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "# Volatility\n",
    "axes[2].plot(features_df.index, features_df['VOLATILITY_LAGGED'], label='Volatility (20d)', color='orange')\n",
    "axes[2].set_title('Market Volatility', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ff179",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Define Multivariate LSTM Model\n",
    "class MultivariateLSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1=64, hidden_size_2=32, \n",
    "                 forecast_horizon=5, dropout=0.2):\n",
    "        super(MultivariateLSTMForecaster, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size_1, batch_first=True, dropout=dropout)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size_1, hidden_size=hidden_size_2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size_2, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, forecast_horizon)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        last_step = out[:, -1, :]\n",
    "        x = self.fc1(last_step)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "print(\"✓ Model class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Prepare Sequences\n",
    "def create_multivariate_sequences(data, target_col_idx, lookback, forecast_horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        X.append(data[i:i+lookback, :])\n",
    "        y.append(data[i+lookback:i+lookback+forecast_horizon, target_col_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "lookback = 20\n",
    "forecast_horizon = 5\n",
    "data_array = features_df.values\n",
    "target_col_idx = 0  # GOLD_LOG_RETURN is first column\n",
    "\n",
    "X, y = create_multivariate_sequences(data_array, target_col_idx, lookback, forecast_horizon)\n",
    "print(f\"Input shape: {X.shape} (samples, lookback, features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7edbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Train/Test Split and Scaling\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "n_features = X.shape[2]\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape for scaling\n",
    "X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test_reshaped).reshape(X_test.shape)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# To Tensor\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train_scaled).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled).to(device)\n",
    "\n",
    "print(\"✓ Data scaled and converted to tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Training Setup\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_size = int(len(train_dataset) * 0.2)\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    train_dataset, [len(train_dataset)-val_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model Init\n",
    "model = MultivariateLSTMForecaster(\n",
    "    input_size=n_features,\n",
    "    hidden_size_1=64, \n",
    "    hidden_size_2=32,\n",
    "    forecast_horizon=forecast_horizon\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12832491",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Training Loop\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_b, y_b in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_b)\n",
    "        loss = criterion(pred, y_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in loader:\n",
    "            pred = model(X_b)\n",
    "            loss = criterion(pred, y_b)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "counter = 0\n",
    "\n",
    "model_dir = '../models/lstm-multivariate-technical'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), f'{model_dir}/best_model.pt')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "            \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Train: {train_loss:.6f} | Val: {val_loss:.6f}\")\n",
    "\n",
    "model.load_state_dict(torch.load(f'{model_dir}/best_model.pt'))\n",
    "print(\"✓ Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Forward-Fill Validation\n",
    "def forward_fill_forecast(model, data_array, train_size, lookback, forecast_horizon, scaler_X, scaler_y, device):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    test_start = train_size + lookback\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(test_start, len(data_array) - forecast_horizon + 1, forecast_horizon):\n",
    "            X_window = data_array[i-lookback:i, :]\n",
    "            X_scaled = scaler_X.transform(X_window.reshape(-1, n_features)).reshape(1, lookback, n_features)\n",
    "            X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "            \n",
    "            y_pred = scaler_y.inverse_transform(model(X_tensor).cpu().numpy())[0]\n",
    "            y_actual = data_array[i:i+forecast_horizon, 0]\n",
    "            \n",
    "            predictions.extend(y_pred)\n",
    "            actuals.extend(y_actual)\n",
    "            \n",
    "    return np.array(predictions), np.array(actuals)\n",
    "\n",
    "print(\"Performing walk-forward validation...\")\n",
    "preds_log, actuals_log = forward_fill_forecast(\n",
    "    model, data_array, train_size, lookback, forecast_horizon, scaler_X, scaler_y, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Convert to Prices\n",
    "predictions_price = []\n",
    "actuals_price = []\n",
    "test_start_idx = train_size + lookback\n",
    "gold_prices = df.loc[features_df.index, 'GOLD_PRICE']\n",
    "\n",
    "for i in range(0, len(preds_log), forecast_horizon):\n",
    "    window_idx = test_start_idx + i\n",
    "    current_price = gold_prices.iloc[window_idx - 1]\n",
    "    \n",
    "    for j in range(forecast_horizon):\n",
    "        if i + j < len(preds_log):\n",
    "            pred_price = current_price * np.exp(preds_log[i+j])\n",
    "            predictions_price.append(pred_price)\n",
    "            current_price = pred_price\n",
    "            actuals_price.append(gold_prices.iloc[window_idx + j])\n",
    "\n",
    "predictions_price = np.array(predictions_price)\n",
    "actuals_price = np.array(actuals_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. Evaluation and Comparison\n",
    "rmse = np.sqrt(mean_squared_error(actuals_price, predictions_price))\n",
    "mae = mean_absolute_error(actuals_price, predictions_price)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL RESULTS: LSTM Technical (No GPRD)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RMSE: ${rmse:.2f}\")\n",
    "print(f\"MAE:  ${mae:.2f}\")\n",
    "\n",
    "# Compare with GPRD model if available\n",
    "try:\n",
    "    gprd_results = pd.read_csv('../models/lstm-multivariate-exogenous/results.csv')\n",
    "    rmse_gprd = gprd_results['rmse'].values[0]\n",
    "    \n",
    "    print(\"\\nComparison with GPRD Model:\")\n",
    "    print(f\"LSTM (Technical Only): ${rmse:.2f}\")\n",
    "    print(f\"LSTM (with GPRD):      ${rmse_gprd:.2f}\")\n",
    "    \n",
    "    diff = rmse - rmse_gprd\n",
    "    if diff > 0:\n",
    "        print(f\"-> GPRD model is better by ${diff:.2f} (RMSE)\")\n",
    "        print(\"Conclusion: Geopolitical risk ADDS predictive value.\")\n",
    "    else:\n",
    "        print(f\"-> Technical model is better by ${-diff:.2f} (RMSE)\")\n",
    "        print(\"Conclusion: GPRD does NOT add predictive value over technicals.\")\n",
    "except:\n",
    "    print(\"\\n(GPRD model results not found for comparison)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13. Save Results\n",
    "results = {\n",
    "    'model': 'LSTM_Multivariate_Technical_Only',\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'features': ', '.join(features_df.columns.tolist())\n",
    "}\n",
    "pd.DataFrame([results]).to_csv(f'{model_dir}/results.csv', index=False)\n",
    "joblib.dump(scaler_X, f'{model_dir}/scaler_X.pkl')\n",
    "joblib.dump(scaler_y, f'{model_dir}/scaler_y.pkl')\n",
    "print(f\"\\nResults saved to {model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
