{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b528894",
   "metadata": {},
   "source": [
    "# LSTM Deep Learning Model\n",
    "\n",
    "**Objective**: Compare deep learning approach with statistical models (ARIMA/GARCH)\n",
    "\n",
    "**Model**: Long Short-Term Memory (LSTM) neural network\n",
    "- **Advantages**: Captures non-linear patterns, learns complex temporal dependencies\n",
    "- **Disadvantages**: Black box, requires more data, computationally expensive\n",
    "\n",
    "**Architecture**: Multi-layer LSTM with dropout for regularization\n",
    "\n",
    "**Use Case**: Alternative to ARIMA when relationships are highly non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b902d0c6",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f3486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0989f8",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/gold_silver.csv')\n",
    "\n",
    "# Convert to datetime\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.sort_values('DATE')\n",
    "df.set_index('DATE', inplace=True)\n",
    "\n",
    "# Calculate log returns\n",
    "df['GOLD_LOG_RETURN'] = np.log(df['GOLD_PRICE']) - np.log(df['GOLD_PRICE'].shift(1))\n",
    "df = df.dropna(subset=['GOLD_LOG_RETURN'])\n",
    "\n",
    "print(f\"Dataset: {len(df)} observations\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70372d89",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering for LSTM\n",
    "\n",
    "Create sequence features (lookback window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lookback window (number of past days to use)\n",
    "lookback = 20  # Use past 20 days to predict next 5\n",
    "forecast_horizon = 5\n",
    "\n",
    "def create_sequences(data, lookback, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM\n",
    "    X: [samples, lookback, features]\n",
    "    y: [samples, forecast_horizon]\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback:i+lookback+forecast_horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Use log returns as target\n",
    "data = df['GOLD_LOG_RETURN'].values\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(data, lookback, forecast_horizon)\n",
    "\n",
    "print(f\"Sequence shape:\")\n",
    "print(f\"  X (input):  {X.shape} - [samples, lookback, features]\")\n",
    "print(f\"  y (output): {y.shape} - [samples, forecast_horizon]\")\n",
    "print(f\"\\nExample: Use {lookback} past days to predict next {forecast_horizon} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a6493",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 split (chronological)\n",
    "train_size = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Train set: {len(X_train)} sequences\")\n",
    "print(f\"Test set:  {len(X_test)} sequences\")\n",
    "\n",
    "# Scale data for neural network (fit on train only)\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape for scaling\n",
    "X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "print(\"\\n✓ Data scaled to [-1, 1] range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917406c",
   "metadata": {},
   "source": [
    "## 5. Build LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da43152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    # First LSTM layer with return sequences\n",
    "    LSTM(64, activation='tanh', return_sequences=True, input_shape=(lookback, 1)),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    LSTM(32, activation='tanh', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    # Output layer (5 values for 5-day forecast)\n",
    "    Dense(forecast_horizon, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"✓ LSTM model built\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7835ec",
   "metadata": {},
   "source": [
    "## 6. Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training LSTM... (this may take 5-10 minutes)\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_scaled,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Model Loss During Training', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_title('Mean Absolute Error During Training', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f06420",
   "metadata": {},
   "source": [
    "## 7. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred_scaled = model.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_original = scaler_y.inverse_transform(y_test_scaled)\n",
    "\n",
    "print(f\"✓ Generated predictions for {len(y_pred)} test sequences\")\n",
    "print(f\"Each prediction contains {forecast_horizon} future values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28b5c7",
   "metadata": {},
   "source": [
    "## 8. Convert to Price Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b08b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert log returns to prices\n",
    "predictions_price = []\n",
    "actuals_price = []\n",
    "\n",
    "# Get test start index\n",
    "test_start_idx = train_size + lookback\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    # Get starting price (last known price before forecast)\n",
    "    start_idx = test_start_idx + i\n",
    "    last_price = df['GOLD_PRICE'].iloc[start_idx - 1]\n",
    "    \n",
    "    # Convert log returns to prices (iterative)\n",
    "    for j in range(forecast_horizon):\n",
    "        pred_price = last_price * np.exp(y_pred[i, j])\n",
    "        actual_price = df['GOLD_PRICE'].iloc[start_idx + j]\n",
    "        \n",
    "        predictions_price.append(pred_price)\n",
    "        actuals_price.append(actual_price)\n",
    "        \n",
    "        last_price = pred_price  # Iterative forecasting\n",
    "\n",
    "print(f\"✓ Converted to {len(predictions_price)} price forecasts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f16ea",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40225d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics on PRICES\n",
    "rmse = np.sqrt(mean_squared_error(actuals_price, predictions_price))\n",
    "mae = mean_absolute_error(actuals_price, predictions_price)\n",
    "\n",
    "# Load previous model results\n",
    "try:\n",
    "    arima_results = pd.read_csv('../models/arima_baseline_results.csv')\n",
    "    arima_garch_results = pd.read_csv('../models/arima_garch_results.csv')\n",
    "    \n",
    "    rmse_arima = arima_results['rmse'].values[0]\n",
    "    mae_arima = arima_results['mae'].values[0]\n",
    "    rmse_garch = arima_garch_results['rmse'].values[0]\n",
    "    mae_garch = arima_garch_results['mae'].values[0]\n",
    "    rmse_naive = arima_results['rmse_naive'].values[0]\n",
    "    mae_naive = arima_results['mae_naive'].values[0]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPREHENSIVE MODEL COMPARISON - 5-DAY AHEAD FORECASTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nLSTM Deep Learning:\")\n",
    "    print(f\"  RMSE: ${rmse:.2f}\")\n",
    "    print(f\"  MAE:  ${mae:.2f}\")\n",
    "    print(f\"\\nARIMA-GARCH Hybrid:\")\n",
    "    print(f\"  RMSE: ${rmse_garch:.2f}\")\n",
    "    print(f\"  MAE:  ${mae_garch:.2f}\")\n",
    "    print(f\"\\nARIMA Baseline:\")\n",
    "    print(f\"  RMSE: ${rmse_arima:.2f}\")\n",
    "    print(f\"  MAE:  ${mae_arima:.2f}\")\n",
    "    print(f\"\\nNaive Benchmark:\")\n",
    "    print(f\"  RMSE: ${rmse_naive:.2f}\")\n",
    "    print(f\"  MAE:  ${mae_naive:.2f}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"LSTM vs Statistical Models:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  vs ARIMA-GARCH: {(1 - rmse/rmse_garch)*100:+.2f}% (RMSE)\")\n",
    "    print(f\"  vs ARIMA:       {(1 - rmse/rmse_arima)*100:+.2f}% (RMSE)\")\n",
    "    print(f\"  vs Naive:       {(1 - rmse/rmse_naive)*100:+.2f}% (RMSE)\")\n",
    "    print(\"=\"*70)\n",
    "except:\n",
    "    print(\"⚠ Previous results not found\")\n",
    "    print(f\"\\nLSTM Results:\")\n",
    "    print(f\"  RMSE: ${rmse:.2f}\")\n",
    "    print(f\"  MAE:  ${mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e5d52",
   "metadata": {},
   "source": [
    "## 10. Visualize LSTM Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Price forecasts\n",
    "forecast_indices = range(len(predictions_price))\n",
    "axes[0].plot(forecast_indices, actuals_price, label='Actual Price', color='black', linewidth=1.5, alpha=0.8)\n",
    "axes[0].plot(forecast_indices, predictions_price, label='LSTM Forecast', color='blue', linewidth=1.5, alpha=0.7)\n",
    "axes[0].set_title('LSTM: Gold Price Forecasts (5-Day Ahead)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Price (USD)', fontsize=11)\n",
    "axes[0].set_xlabel('Forecast Step', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Forecast errors\n",
    "errors = np.array(actuals_price) - np.array(predictions_price)\n",
    "axes[1].plot(forecast_indices, errors, color='red', linewidth=1)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].fill_between(forecast_indices, errors, 0, alpha=0.3, color='red')\n",
    "axes[1].set_title('Forecast Errors', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Error (USD)', fontsize=11)\n",
    "axes[1].set_xlabel('Forecast Step', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122dbb6",
   "metadata": {},
   "source": [
    "## 11. Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c522f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(errors, bins=50, edgecolor='black', alpha=0.7, color='blue')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_title('Distribution of Forecast Errors', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Error (USD)', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(errors, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Error Statistics:\")\n",
    "print(f\"  Mean:     ${np.mean(errors):.2f}\")\n",
    "print(f\"  Std Dev:  ${np.std(errors):.2f}\")\n",
    "print(f\"  Skewness: {stats.skew(errors):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262788e6",
   "metadata": {},
   "source": [
    "## 12. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving results\n",
    "import os\n",
    "model_dir = '../models/lstm-deep-learning'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'model': 'LSTM',\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'n_predictions': len(predictions_price),\n",
    "    'lookback': lookback,\n",
    "    'forecast_horizon': forecast_horizon,\n",
    "    'lstm_units_1': 64,\n",
    "    'lstm_units_2': 32\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(f'{model_dir}/results.csv', index=False)\n",
    "\n",
    "# Save model\n",
    "model.save(f'{model_dir}/model.h5')\n",
    "\n",
    "# Save scalers\n",
    "import joblib\n",
    "joblib.dump(scaler_X, f'{model_dir}/scaler_X.pkl')\n",
    "joblib.dump(scaler_y, f'{model_dir}/scaler_y.pkl')\n",
    "\n",
    "print(f\"✓ Model and results saved to '{model_dir}/'\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - results.csv\")\n",
    "print(\"  - model.h5\")\n",
    "print(\"  - scaler_X.pkl\")\n",
    "print(\"  - scaler_y.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438f3a0",
   "metadata": {},
   "source": [
    "## 13. Final Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea337a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "try:\n",
    "    comparison_data = {\n",
    "        'Model': ['Naive Baseline', 'ARIMA', 'ARIMA-GARCH', 'LSTM'],\n",
    "        'RMSE ($)': [rmse_naive, rmse_arima, rmse_garch, rmse],\n",
    "        'MAE ($)': [mae_naive, mae_arima, mae_garch, mae],\n",
    "        'Type': ['Statistical', 'Statistical', 'Statistical', 'Deep Learning'],\n",
    "        'Volatility Forecast': ['No', 'No', 'Yes', 'No'],\n",
    "        'Exogenous Variables': ['No', 'No', 'No', 'No'],\n",
    "        'Interpretability': ['High', 'High', 'Medium', 'Low']\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('RMSE ($)')\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL MODEL COMPARISON - GOLDENHOUR PROJECT\")\n",
    "    print(\"=\"*80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_dir = '../models/comparison'\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    comparison_df.to_csv(f'{comparison_dir}/model_comparison.csv', index=False)\n",
    "    print(f\"\\n✓ Comparison saved to {comparison_dir}/model_comparison.csv\")\n",
    "except:\n",
    "    print(\"⚠ Could not create full comparison (missing previous results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca726805",
   "metadata": {},
   "source": [
    "## 14. Key Findings\n",
    "\n",
    "**LSTM Characteristics**:\n",
    "- **Strengths**:\n",
    "  - Captures non-linear patterns\n",
    "  - No stationarity assumptions\n",
    "  - Can handle complex temporal dependencies\n",
    "  \n",
    "- **Weaknesses**:\n",
    "  - Black box (low interpretability)\n",
    "  - Requires substantial training data\n",
    "  - Computationally expensive\n",
    "  - Prone to overfitting\n",
    "  - No theoretical foundation (unlike ARIMA)\n",
    "\n",
    "**When to Use LSTM vs ARIMA**:\n",
    "- **Use LSTM**: Complex non-linear relationships, large datasets, computational resources available\n",
    "- **Use ARIMA**: Interpretability needed, smaller datasets, academic rigor required, statistical inference desired\n",
    "\n",
    "**For Academic Project**: ARIMA-GARCH recommended due to:\n",
    "1. Solid theoretical foundation\n",
    "2. Interpretable coefficients\n",
    "3. Volatility forecasting capability\n",
    "4. Well-documented in econometrics literature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
