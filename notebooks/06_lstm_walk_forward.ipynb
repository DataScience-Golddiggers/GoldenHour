{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1314048",
   "metadata": {},
   "source": [
    "# LSTM with Walk-Forward Validation\n",
    "\n",
    "**Objective**: Implement LSTM deep learning model with proper walk-forward validation\n",
    "\n",
    "**Key Differences from Standard LSTM**:\n",
    "- ✅ **Walk-forward validation**: Re-train model incrementally on expanding window\n",
    "- ✅ **Realistic forecasting**: Simulates real-world deployment scenario\n",
    "- ✅ **No future data leakage**: Model only sees past data at each prediction step\n",
    "\n",
    "**Training Strategy**:\n",
    "1. Start with initial training window (e.g., 80% of data)\n",
    "2. Predict next 5 days\n",
    "3. Add actual observed data to training set\n",
    "4. Re-train model (or update with new data)\n",
    "5. Repeat until end of dataset\n",
    "\n",
    "**Trade-offs**:\n",
    "- ⚠️ **Computationally expensive**: Multiple model trainings required\n",
    "- ⚠️ **Time-consuming**: Can take hours depending on dataset size\n",
    "- ✅ **More realistic**: Better reflects production performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f328d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f304c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T12:56:59.110244Z",
     "start_time": "2025-11-03T12:56:58.960513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "TensorFlow version: 2.16.1\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826eced6",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d232de47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10570 observations\n",
      "Date range: 1985-01-03 00:00:00 to 2025-09-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/gold_silver.csv')\n",
    "\n",
    "# Convert to datetime\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.sort_values('DATE')\n",
    "df.set_index('DATE', inplace=True)\n",
    "\n",
    "# Calculate log returns\n",
    "df['GOLD_LOG_RETURN'] = np.log(df['GOLD_PRICE']) - np.log(df['GOLD_PRICE'].shift(1))\n",
    "df = df.dropna(subset=['GOLD_LOG_RETURN'])\n",
    "\n",
    "print(f\"Dataset: {len(df)} observations\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc30d9",
   "metadata": {},
   "source": [
    "## 3. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c86d1896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk-Forward Configuration:\n",
      "  Initial training size: 7398 observations\n",
      "  Lookback window: 20 days\n",
      "  Forecast horizon: 5 days\n",
      "  Retrain frequency: every 20 observations\n",
      "\n",
      "Test set size: 3152 observations\n",
      "Expected retraining cycles: ~157\n"
     ]
    }
   ],
   "source": [
    "# Walk-forward validation parameters\n",
    "lookback = 20  # Past days to use for prediction\n",
    "forecast_horizon = 5  # Days ahead to forecast\n",
    "initial_train_size = int(len(df) * 0.7)  # Start with 70% for initial training\n",
    "retrain_frequency = 20  # Re-train model every N observations\n",
    "\n",
    "# Model parameters\n",
    "lstm_units_1 = 64\n",
    "lstm_units_2 = 32\n",
    "dense_units = 16\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "epochs = 50  # Reduced for faster iteration\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Walk-Forward Configuration:\")\n",
    "print(f\"  Initial training size: {initial_train_size} observations\")\n",
    "print(f\"  Lookback window: {lookback} days\")\n",
    "print(f\"  Forecast horizon: {forecast_horizon} days\")\n",
    "print(f\"  Retrain frequency: every {retrain_frequency} observations\")\n",
    "print(f\"\\nTest set size: {len(df) - initial_train_size - lookback} observations\")\n",
    "print(f\"Expected retraining cycles: ~{(len(df) - initial_train_size - lookback) // retrain_frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c162c1b",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65cb9fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, lookback, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM\n",
    "    X: [samples, lookback, features]\n",
    "    y: [samples, forecast_horizon]\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback:i+lookback+forecast_horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_lstm_model(lookback, forecast_horizon, lstm_units_1, lstm_units_2, dense_units, dropout_rate, learning_rate):\n",
    "    \"\"\"\n",
    "    Build and compile LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units_1, activation='tanh', return_sequences=True, input_shape=(lookback, 1)),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units_2, activation='tanh', return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate/2),\n",
    "        Dense(forecast_horizon, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def scale_data(X_train, y_train, X_val=None, y_val=None):\n",
    "    \"\"\"\n",
    "    Scale data using MinMaxScaler fitted on training data\n",
    "    \"\"\"\n",
    "    scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "    \n",
    "    X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    \n",
    "    if X_val is not None and y_val is not None:\n",
    "        X_val_scaled = scaler_X.transform(X_val.reshape(-1, 1)).reshape(X_val.shape)\n",
    "        y_val_scaled = scaler_y.transform(y_val)\n",
    "        return X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, scaler_X, scaler_y\n",
    "    \n",
    "    return X_train_scaled, y_train_scaled, scaler_X, scaler_y\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ec88b",
   "metadata": {},
   "source": [
    "## 5. Walk-Forward Validation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43fb028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING WALK-FORWARD VALIDATION\n",
      "================================================================================\n",
      "⚠️  This process may take 30-60 minutes depending on your hardware\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m steps_completed = \u001b[32m0\u001b[39m\n\u001b[32m     22\u001b[39m total_steps = \u001b[38;5;28mlen\u001b[39m(df) - initial_train_size - lookback - forecast_horizon + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookback\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m              \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWalk-Forward Progress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# 1. Extract training data (expanding window)\u001b[39;00m\n\u001b[32m     28\u001b[39m     train_data = log_returns[:i]\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# 2. Create sequences\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Università/UnivPM/Data Science/Tesina 2/GoldenHour/.venv/lib/python3.12/site-packages/tqdm/notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Università/UnivPM/Data Science/Tesina 2/GoldenHour/.venv/lib/python3.12/site-packages/tqdm/notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Extract log returns\n",
    "log_returns = df['GOLD_LOG_RETURN'].values\n",
    "prices = df['GOLD_PRICE'].values\n",
    "\n",
    "# Storage for predictions and actuals\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "forecast_dates = []\n",
    "\n",
    "# Initialize model (will be retrained during walk-forward)\n",
    "model = None\n",
    "scaler_X = None\n",
    "scaler_y = None\n",
    "\n",
    "# Walk-forward loop\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING WALK-FORWARD VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"⚠️  This process may take 30-60 minutes depending on your hardware\\n\")\n",
    "\n",
    "steps_completed = 0\n",
    "total_steps = len(df) - initial_train_size - lookback - forecast_horizon + 1\n",
    "\n",
    "for i in tqdm(range(initial_train_size, len(df) - lookback - forecast_horizon + 1), \n",
    "              desc=\"Walk-Forward Progress\"):\n",
    "    \n",
    "    # 1. Extract training data (expanding window)\n",
    "    train_data = log_returns[:i]\n",
    "    \n",
    "    # 2. Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, lookback, forecast_horizon)\n",
    "    \n",
    "    # 3. Check if we need to retrain\n",
    "    should_retrain = (model is None) or (steps_completed % retrain_frequency == 0)\n",
    "    \n",
    "    if should_retrain:\n",
    "        # Scale data\n",
    "        X_train_scaled, y_train_scaled, scaler_X, scaler_y = scale_data(X_train, y_train)\n",
    "        \n",
    "        # Build new model\n",
    "        model = build_lstm_model(lookback, forecast_horizon, lstm_units_1, \n",
    "                                lstm_units_2, dense_units, dropout_rate, learning_rate)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_scaled,\n",
    "            y_train_scaled,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    # 4. Prepare input for prediction (last 'lookback' days)\n",
    "    X_pred = log_returns[i-lookback:i].reshape(1, lookback, 1)\n",
    "    X_pred_scaled = scaler_X.transform(X_pred.reshape(-1, 1)).reshape(X_pred.shape)\n",
    "    \n",
    "    # 5. Make prediction\n",
    "    y_pred_scaled = model.predict(X_pred_scaled, verbose=0)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    # 6. Get actual values\n",
    "    y_actual = log_returns[i:i+forecast_horizon]\n",
    "    \n",
    "    # 7. Convert log returns to prices\n",
    "    last_known_price = prices[i-1]\n",
    "    \n",
    "    for j in range(forecast_horizon):\n",
    "        # Predict price using log return\n",
    "        pred_price = last_known_price * np.exp(y_pred[0, j])\n",
    "        actual_price = prices[i + j]\n",
    "        \n",
    "        all_predictions.append(pred_price)\n",
    "        all_actuals.append(actual_price)\n",
    "        forecast_dates.append(df.index[i + j])\n",
    "        \n",
    "        # Use ACTUAL price for next step (no look-ahead bias in evaluation)\n",
    "        if j < forecast_horizon - 1:\n",
    "            last_known_price = actual_price\n",
    "    \n",
    "    steps_completed += 1\n",
    "\n",
    "print(f\"\\n✓ Walk-forward validation completed!\")\n",
    "print(f\"  Total forecasts generated: {len(all_predictions)}\")\n",
    "print(f\"  Models trained: {steps_completed // retrain_frequency + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f38e7",
   "metadata": {},
   "source": [
    "## 6. Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b413fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(all_actuals, all_predictions))\n",
    "mae = mean_absolute_error(all_actuals, all_predictions)\n",
    "\n",
    "# Calculate naive baseline for comparison\n",
    "naive_predictions = []\n",
    "for i in range(len(forecast_dates)):\n",
    "    # Find the corresponding date in original dataframe\n",
    "    date_idx = df.index.get_loc(forecast_dates[i])\n",
    "    naive_predictions.append(df['GOLD_PRICE'].iloc[date_idx - 1])  # Previous day's price\n",
    "\n",
    "rmse_naive = np.sqrt(mean_squared_error(all_actuals, naive_predictions))\n",
    "mae_naive = mean_absolute_error(all_actuals, naive_predictions)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WALK-FORWARD LSTM RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLSTM Walk-Forward:\")\n",
    "print(f\"  RMSE: ${rmse:.2f}\")\n",
    "print(f\"  MAE:  ${mae:.2f}\")\n",
    "print(f\"\\nNaive Baseline:\")\n",
    "print(f\"  RMSE: ${rmse_naive:.2f}\")\n",
    "print(f\"  MAE:  ${mae_naive:.2f}\")\n",
    "print(f\"\\nImprovement vs Naive:\")\n",
    "print(f\"  RMSE: {(1 - rmse/rmse_naive)*100:+.2f}%\")\n",
    "print(f\"  MAE:  {(1 - mae/mae_naive)*100:+.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load previous model results for comparison\n",
    "try:\n",
    "    lstm_basic_results = pd.read_csv('../models/lstm-deep-learning/results.csv')\n",
    "    arima_results = pd.read_csv('../models/arima-baseline/results.csv')\n",
    "    arima_garch_results = pd.read_csv('../models/arima-garch-hybrid/results.csv')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON WITH OTHER MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nLSTM Walk-Forward:  RMSE=${rmse:.2f}, MAE=${mae:.2f}\")\n",
    "    print(f\"LSTM Basic:         RMSE=${lstm_basic_results['rmse'].values[0]:.2f}, MAE=${lstm_basic_results['mae'].values[0]:.2f}\")\n",
    "    print(f\"ARIMA-GARCH:        RMSE=${arima_garch_results['rmse'].values[0]:.2f}, MAE=${arima_garch_results['mae'].values[0]:.2f}\")\n",
    "    print(f\"ARIMA Baseline:     RMSE=${arima_results['rmse'].values[0]:.2f}, MAE=${arima_results['mae'].values[0]:.2f}\")\n",
    "    print(\"=\"*80)\n",
    "except:\n",
    "    print(\"\\n⚠ Could not load previous model results for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace48897",
   "metadata": {},
   "source": [
    "## 7. Visualize Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Price forecasts over time\n",
    "axes[0].plot(forecast_dates, all_actuals, label='Actual Price', color='black', linewidth=1.5, alpha=0.8)\n",
    "axes[0].plot(forecast_dates, all_predictions, label='LSTM Walk-Forward Forecast', color='blue', linewidth=1, alpha=0.7)\n",
    "axes[0].set_title('LSTM Walk-Forward: Gold Price Forecasts (5-Day Ahead)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Price (USD)', fontsize=11)\n",
    "axes[0].set_xlabel('Date', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Forecast errors\n",
    "errors = np.array(all_actuals) - np.array(all_predictions)\n",
    "axes[1].plot(forecast_dates, errors, color='red', linewidth=1, alpha=0.7)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1.5)\n",
    "axes[1].fill_between(forecast_dates, errors, 0, alpha=0.3, color='red')\n",
    "axes[1].set_title('Forecast Errors Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Error (USD)', fontsize=11)\n",
    "axes[1].set_xlabel('Date', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nForecast Period: {forecast_dates[0].strftime('%Y-%m-%d')} to {forecast_dates[-1].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88c82c",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(errors, bins=60, edgecolor='black', alpha=0.7, color='blue')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_title('Distribution of Forecast Errors', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Error (USD)', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(errors, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Absolute errors over time\n",
    "abs_errors = np.abs(errors)\n",
    "axes[2].plot(forecast_dates, abs_errors, color='orange', linewidth=1, alpha=0.7)\n",
    "axes[2].axhline(y=mae, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mae:.2f}')\n",
    "axes[2].set_title('Absolute Errors Over Time', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Date', fontsize=11)\n",
    "axes[2].set_ylabel('Absolute Error (USD)', fontsize=11)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nError Statistics:\")\n",
    "print(f\"  Mean:     ${np.mean(errors):.2f}\")\n",
    "print(f\"  Std Dev:  ${np.std(errors):.2f}\")\n",
    "print(f\"  Median:   ${np.median(errors):.2f}\")\n",
    "print(f\"  Min:      ${np.min(errors):.2f}\")\n",
    "print(f\"  Max:      ${np.max(errors):.2f}\")\n",
    "print(f\"  Skewness: {stats.skew(errors):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(errors):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8fce2",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86739b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving results\n",
    "model_dir = '../models/lstm-walk-forward'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'model': 'LSTM-WalkForward',\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'rmse_naive': rmse_naive,\n",
    "    'mae_naive': mae_naive,\n",
    "    'n_predictions': len(all_predictions),\n",
    "    'lookback': lookback,\n",
    "    'forecast_horizon': forecast_horizon,\n",
    "    'initial_train_size': initial_train_size,\n",
    "    'retrain_frequency': retrain_frequency,\n",
    "    'lstm_units_1': lstm_units_1,\n",
    "    'lstm_units_2': lstm_units_2,\n",
    "    'epochs': epochs\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(f'{model_dir}/results.csv', index=False)\n",
    "\n",
    "# Save detailed predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'date': forecast_dates,\n",
    "    'actual': all_actuals,\n",
    "    'predicted': all_predictions,\n",
    "    'error': errors,\n",
    "    'abs_error': abs_errors\n",
    "})\n",
    "predictions_df.to_csv(f'{model_dir}/predictions.csv', index=False)\n",
    "\n",
    "# Save final model\n",
    "model.save(f'{model_dir}/model_final.h5')\n",
    "\n",
    "print(f\"✓ Results saved to '{model_dir}/'\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - results.csv (summary metrics)\")\n",
    "print(\"  - predictions.csv (detailed forecasts)\")\n",
    "print(\"  - model_final.h5 (final trained model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846dafe",
   "metadata": {},
   "source": [
    "## 10. Final Comparison with All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cda9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "try:\n",
    "    # Load all model results\n",
    "    lstm_basic = pd.read_csv('../models/lstm-deep-learning/results.csv')\n",
    "    arima_baseline = pd.read_csv('../models/arima-baseline/results.csv')\n",
    "    arima_garch = pd.read_csv('../models/arima-garch-hybrid/results.csv')\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Model': ['Naive Baseline', 'ARIMA', 'ARIMA-GARCH', 'LSTM (Basic)', 'LSTM (Walk-Forward)'],\n",
    "        'RMSE ($)': [\n",
    "            rmse_naive,\n",
    "            arima_baseline['rmse'].values[0],\n",
    "            arima_garch['rmse'].values[0],\n",
    "            lstm_basic['rmse'].values[0],\n",
    "            rmse\n",
    "        ],\n",
    "        'MAE ($)': [\n",
    "            mae_naive,\n",
    "            arima_baseline['mae'].values[0],\n",
    "            arima_garch['mae'].values[0],\n",
    "            lstm_basic['mae'].values[0],\n",
    "            mae\n",
    "        ],\n",
    "        'Type': ['Statistical', 'Statistical', 'Statistical', 'Deep Learning', 'Deep Learning'],\n",
    "        'Validation': ['N/A', 'Walk-Forward', 'Walk-Forward', 'Train-Test Split', 'Walk-Forward'],\n",
    "        'Volatility': ['No', 'No', 'Yes', 'No', 'No'],\n",
    "        'Interpretability': ['High', 'High', 'Medium', 'Low', 'Low']\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('RMSE ($)')\n",
    "    \n",
    "    print(\"=\"*90)\n",
    "    print(\"FINAL MODEL COMPARISON - GOLDENHOUR PROJECT\")\n",
    "    print(\"=\"*90)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_dir = '../models/comparison'\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    comparison_df.to_csv(f'{comparison_dir}/model_comparison_full.csv', index=False)\n",
    "    print(f\"\\n✓ Comparison saved to {comparison_dir}/model_comparison_full.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not create full comparison: {e}\")\n",
    "    print(\"\\nLSTM Walk-Forward Results:\")\n",
    "    print(f\"  RMSE: ${rmse:.2f}\")\n",
    "    print(f\"  MAE:  ${mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7aae18",
   "metadata": {},
   "source": [
    "## 11. Key Findings\n",
    "\n",
    "### Walk-Forward Validation Benefits\n",
    "\n",
    "**Advantages**:\n",
    "1. ✅ **Realistic Performance**: Simulates real-world deployment scenario\n",
    "2. ✅ **No Future Data Leakage**: Model only sees past data at each step\n",
    "3. ✅ **Adapts to Regime Changes**: Re-training captures market dynamics\n",
    "4. ✅ **More Reliable Estimates**: Better reflects production performance\n",
    "\n",
    "**Disadvantages**:\n",
    "1. ⚠️ **Computationally Expensive**: Multiple model trainings required\n",
    "2. ⚠️ **Time-Consuming**: Can take hours for large datasets\n",
    "3. ⚠️ **Complex Implementation**: More difficult to debug and maintain\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "**Expected Outcome**: Walk-forward LSTM should show:\n",
    "- **Higher errors** than basic train-test split (more realistic)\n",
    "- **More stable predictions** over time\n",
    "- **Better generalization** to unseen market conditions\n",
    "\n",
    "### Recommendations for Academic Project\n",
    "\n",
    "**For thesis/paper**:\n",
    "1. **Primary Model**: ARIMA-GARCH (theoretical foundation, interpretability)\n",
    "2. **Comparison Model**: LSTM Walk-Forward (demonstrate modern techniques)\n",
    "3. **Baseline**: Naive random walk (industry standard benchmark)\n",
    "\n",
    "**Justification**:\n",
    "- ARIMA-GARCH: Established econometric methodology, widely accepted in academia\n",
    "- LSTM Walk-Forward: Shows awareness of ML best practices and realistic evaluation\n",
    "- Both use walk-forward validation for fair comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
